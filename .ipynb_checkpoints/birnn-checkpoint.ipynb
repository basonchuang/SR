{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f13959d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import ctc_ops\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bc375c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "used to create a variable in CPU memory.\n",
    "\"\"\"\n",
    "def variable_on_cpu(name, shape, initializer):\n",
    "    # Use the /cpu:0 device for scoped operations\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Create or get apropos variable\n",
    "        var = tf.get_variable(name=name, shape=shape, initializer=initializer)\n",
    "    return var\n",
    "\n",
    "class BiRNN():\n",
    "    def __init__(self, features, contexts, batch_size, hidden, cell_dim, stddev, keep_dropout_rate, relu_clip, character, save_path, learning_rate):\n",
    "        self.features = features\n",
    "        self.batch_size = batch_size\n",
    "        self.contexts = contexts\n",
    "        self.hidden = hidden\n",
    "        self.stddev = stddev\n",
    "        self.keep_dropout_rate = keep_dropout_rate\n",
    "        self.relu_clip = relu_clip\n",
    "        self.cell_dim = cell_dim\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # input 为输入音频数据，由前面分析可知，它的结构是[batch_size, amax_stepsize, features + (2 * features * contexts)]\n",
    "        #其中，batch_size是batch的长度，amax_stepsize是时序长度，n_input + (2 * features * contexts)是MFCC特征数，\n",
    "        #batch_size是可变的，所以设为None，由于每一批次的时序长度不固定，所有，amax_stepsize也设为None\n",
    "        self.input = tf.placeholder(tf.float32, [None, None, features + (2 * features * contexts)], name='input')\n",
    "       \n",
    "        # label 保存的是音频数据对应的文本的系数张量，所以用sparse_placeholder创建一个稀疏张量\n",
    "        self.label = tf.sparse_placeholder(tf.int32, name='label')\n",
    "\n",
    "        #seq_length保存的是当前batch数据的时序长度\n",
    "        self.seq_length = tf.placeholder(tf.int32, [None], name='seq_length')\n",
    "\n",
    "        #keep_dropout则是dropout的参数\n",
    "        self.keep_dropout = tf.placeholder(tf.float32, name='keep_dropout')\n",
    "        \n",
    "        self.network_init(self.input, character)\n",
    "        self.loss_init()\n",
    "        self.optimizer_init()\n",
    "        self.accuracy_init()\n",
    "\n",
    "        #创建会话\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        #需要保存模型，所以获取saver\n",
    "        self.saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "        #模型保存地址\n",
    "        self.save_path = save_path\n",
    "        #如果该目录不存在，新建\n",
    "        if os.path.exists(self.save_path) == False:\n",
    "            os.mkdir(self.save_path)\n",
    "\n",
    "        #初始化\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # 没有模型的话，就重新初始化\n",
    "        cpkt = tf.train.latest_checkpoint(self.save_path)\n",
    "        \n",
    "        self.start_epoch = 0\n",
    "        if cpkt != None:\n",
    "            self.saver.restore(self.sess, cpkt)\n",
    "            ind = cpkt.find(\"-\")\n",
    "            self.start_epoch = int(cpkt[ind + 1:])\n",
    "\n",
    "    def get_property(self):\n",
    "        return self.start_epoch\n",
    "\n",
    "    def network_init(self, input, character):\n",
    "        # batch_x_shape: [batch_size, amax_stepsize, n_input + 2 * n_input * contexts]\n",
    "        batch_x_shape = tf.shape(input)\n",
    "    \n",
    "        # 将输入转成时间序列优先\n",
    "        input = tf.transpose(input, [1, 0, 2])\n",
    "        # 再转成2维传入第一层\n",
    "        # [amax_stepsize * batch_size, n_input + 2 * n_input * contexts]\n",
    "        input = tf.reshape(input, [-1, self.features + 2 * self.features * self.contexts])\n",
    "        \n",
    "        # 使用clipped RELU activation and dropout.\n",
    "        # 1st layer\n",
    "        with tf.name_scope('fc1'):\n",
    "            b1 = variable_on_cpu('b1', [self.hidden], tf.random_normal_initializer(stddev=self.stddev))        \n",
    "            h1 = variable_on_cpu('h1', [self.features + 2 * self.features * self.contexts, self.hidden],\n",
    "                                tf.random_normal_initializer(stddev=self.stddev))\n",
    "            layer_1 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(input, h1), b1)), self.relu_clip)\n",
    "            layer_1 = tf.nn.dropout(layer_1, self.keep_dropout)\n",
    "        \n",
    "        # 2nd layer\n",
    "        with tf.name_scope('fc2'):\n",
    "            b2 = variable_on_cpu('b2', [self.hidden], tf.random_normal_initializer(stddev=self.stddev))\n",
    "            h2 = variable_on_cpu('h2', [self.hidden, self.hidden], tf.random_normal_initializer(stddev=self.stddev))\n",
    "            layer_2 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(layer_1, h2), b2)), self.relu_clip)\n",
    "            layer_2 = tf.nn.dropout(layer_2, self.keep_dropout)\n",
    "    \n",
    "        # 3rd layer\n",
    "        with tf.name_scope('fc3'):\n",
    "            b3 = variable_on_cpu('b3', [2 * self.hidden], tf.random_normal_initializer(stddev=self.stddev))\n",
    "            h3 = variable_on_cpu('h3', [self.hidden, 2 * self.hidden], tf.random_normal_initializer(stddev=self.stddev))\n",
    "            layer_3 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(layer_2, h3), b3)), self.relu_clip)\n",
    "            layer_3 = tf.nn.dropout(layer_3, self.keep_dropout)\n",
    "    \n",
    "        # 双向rnn\n",
    "        with tf.name_scope('lstm'):\n",
    "            # Forward direction cell:\n",
    "            lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(self.cell_dim, forget_bias=1.0, state_is_tuple=True)\n",
    "            lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(lstm_fw_cell,\n",
    "                                                        input_keep_prob=self.keep_dropout)\n",
    "            # Backward direction cell:\n",
    "            lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(self.cell_dim, forget_bias=1.0, state_is_tuple=True)\n",
    "            lstm_bw_cell = tf.contrib.rnn.DropoutWrapper(lstm_bw_cell,\n",
    "                                                        input_keep_prob=self.keep_dropout)\n",
    "    \n",
    "            # `layer_3`  `[amax_stepsize, batch_size, 2 * cell_dim]`\n",
    "            layer_3 = tf.reshape(layer_3, [-1, batch_x_shape[0], 2 * self.cell_dim])\n",
    "    \n",
    "            outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell,\n",
    "                                                                    cell_bw=lstm_bw_cell,\n",
    "                                                                    inputs=layer_3,\n",
    "                                                                    dtype=tf.float32,\n",
    "                                                                    time_major=True,\n",
    "                                                                    sequence_length=self.seq_length)\n",
    "    \n",
    "            # 连接正反向结果[amax_stepsize, batch_size, 2 * n_cell_dim]\n",
    "            outputs = tf.concat(outputs, 2)\n",
    "            # to a single tensor of shape [amax_stepsize * batch_size, 2 * n_cell_dim]\n",
    "            outputs = tf.reshape(outputs, [-1, 2 * self.hidden])\n",
    "    \n",
    "        with tf.name_scope('fc5'):\n",
    "            b5 = variable_on_cpu('b5', [self.hidden], tf.random_normal_initializer(stddev=self.stddev))\n",
    "            h5 = variable_on_cpu('h5', [(2 * self.hidden), self.hidden], tf.random_normal_initializer(stddev=self.stddev))\n",
    "            layer_5 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(outputs, h5), b5)), self.relu_clip)\n",
    "            layer_5 = tf.nn.dropout(layer_5, self.keep_dropout)\n",
    "    \n",
    "        with tf.name_scope('fc6'):\n",
    "            # 全连接层用于softmax分类\n",
    "            b6 = variable_on_cpu('b6', [character], tf.random_normal_initializer(stddev=self.stddev))\n",
    "            h6 = variable_on_cpu('h6', [self.hidden, character], tf.random_normal_initializer(stddev=self.stddev))\n",
    "            layer_6 = tf.add(tf.matmul(layer_5, h6), b6)\n",
    "    \n",
    "        # 将2维[amax_stepsize * batch_size, character]转成3维 time-major [amax_stepsize, batch_size, character].        \n",
    "        self.pred = tf.reshape(layer_6, [-1, batch_x_shape[0], character], name='pred')        \n",
    "               \n",
    "    #损失函数\n",
    "    def loss_init(self):\n",
    "        # 使用ctc loss计算损失\n",
    "        self.loss = tf.reduce_mean(ctc_ops.ctc_loss(self.label, self.pred, self.seq_length))\n",
    "\n",
    "    #优化器\n",
    "    def optimizer_init(self):\n",
    "        # 优化器        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "\n",
    "    def accuracy_init(self):\n",
    "        # 使用CTC decoder\n",
    "        with tf.name_scope(\"decode\"):\n",
    "            self.decoded, _ = ctc_ops.ctc_beam_search_decoder(self.pred, self.seq_length, merge_repeated=False)\n",
    "            \n",
    "        # 计算编辑距离\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            distance = tf.edit_distance(tf.cast(self.decoded[0], tf.int32), self.label)\n",
    "            # 计算label error rate (accuracy)\n",
    "            self.label_error_rate = tf.reduce_mean(distance, name='label_error_rate')\n",
    "\n",
    "\n",
    "    def run(self, batch, source, source_lengths, sparse_labels, words, epoch):\n",
    "        feed = {self.input: source, self.seq_length: source_lengths, self.label: sparse_labels, \n",
    "                    self.keep_dropout: self.keep_dropout_rate}\n",
    "\n",
    "        # loss optimizer ;\n",
    "        loss, _ = self.sess.run([self.loss, self.optimizer], feed_dict=feed)\n",
    "        \n",
    "        # 验证模型的准确率，比较耗时，我们训练的时候全力以赴，所以这里先不跑\n",
    "        # if (batch + 1) % 1 == 0:            \n",
    "        #     feed2 = {self.input: source, self.seq_length: source_lengths, self.label: sparse_labels, self.keep_dropout: 1.0}        \n",
    "        #     decoded, label_error_rate = self.sess.run([self.decoded[0], self.label_error_rate], feed_dict=feed2)        \n",
    "        #     dense_decodeds = tf.sparse_tensor_to_dense(decoded, default_value=0).eval(session=self.sess)\n",
    "        #     dense_original_labels = sparse_tuple_to_text(sparse_labels, words)        \n",
    "        #     counter = 0            \n",
    "        #     print('Label err rate: ', label_error_rate)\n",
    "        #     for dense_original_label, dense_decoded in zip(dense_original_labels, dense_decodeds):\n",
    "        #         # convert to strings\n",
    "        #         decoded_str = dense_to_text(dense_decoded, words)                 \n",
    "        #         print('Original: {}'.format(dense_original_label))\n",
    "        #         print('Decoded:  {}'.format(decoded_str))\n",
    "        #         print('------------------------------------------')\n",
    "        #         counter = counter + 1\n",
    "                \n",
    "\n",
    "        #每训练100次保存一下模型\n",
    "        if (batch + 1) % 100 == 0:\n",
    "            self.saver.save(self.sess, os.path.join(self.save_path + \"birnn_speech_recognition.cpkt\"), global_step=epoch)\n",
    "\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SR",
   "language": "python",
   "name": "sr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
